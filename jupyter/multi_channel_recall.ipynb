{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241716e0bc5f45e9",
   "metadata": {},
   "source": [
    "# 多路召回\n",
    "\n",
    "所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用，可以明显的看出，“多路召回策略”是在“计算速度”和“召回率”之间进行权衡的结果。\n",
    "\n",
    "其中，各种简单策略保证候选集的快速召回，从不同角度设计的策略保证召回率接近理想的状态，不至于损伤排序效果。如下图是多路召回的一个示意图，在多路召回中，每个策略之间毫不相关，所以一般可以写并发多线程同时进行，这样可以更加高效。\n",
    "\n",
    "<img src=\"../image/architecture.png\">\n",
    "\n",
    "上图只是一个多路召回的例子，也就是说可以使用多种不同的策略来获取用户排序的候选商品集合，而具体使用哪些召回策略其实是与业务强相关的 ，针对不同的任务就会有对于该业务真实场景下需要考虑的召回规则。例如新闻推荐，召回规则可以是“热门视频”、“导演召回”、“演员召回”、“最近上映“、”流行趋势“、”类型召回“等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed28da65cf2219b",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614cbb4809ee850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:09:13.834516Z",
     "start_time": "2024-11-22T01:09:10.821131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "DeepMatch version 0.3.1 detected. Your version is 0.2.0.\n",
      "Use `pip install -U deepmatch` to upgrade.Changelog: https://github.com/shenweichen/DeepMatch/releases/tag/v0.3.1\n",
      "WARNING:root:\n",
      "DeepCTR version 0.9.3 detected. Your version is 0.8.2.\n",
      "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.9.3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import faiss\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "\n",
    "from deepmatch.models import YoutubeDNN\n",
    "from deepmatch.utils import sampledsoftmaxloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8305d19219392f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:35:14.985151Z",
     "start_time": "2024-11-21T01:35:14.971538Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "save_path = '../tmp_results/'\n",
    "# 做召回评估的一个标志，如果不进行评估就是直接使用全量数据进行召回\n",
    "metric_recall = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d479aa1a1cbbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2 读取数据\n",
    "\n",
    "在一般的推荐系统比赛中读取数据部分主要分为三种模式，不同的模式对应的不同的数据集：\n",
    "\n",
    "1. Debug模式：这个的目的是帮助我们基于数据先搭建一个简易的 baseline 并跑通，保证写的 baseline 代码没有什么问题。由于推荐比赛的数据往往非常巨大，如果一上来直接采用全部的数据进行分析，搭建baseline框架，往往会带来时间和设备上的损耗，所以这时候我们往往需要从海量数据的训练集中随机抽取一部分样本来进行调试（train_click_log_sample），先跑通一个 baseline。\n",
    "2. 线下验证模式：这个的目的是帮助我们在线下基于已有的训练集数据，来选择好合适的模型和一些超参数。所以我们这一块只需要加载整个训练集（train_click_log），然后把整个训练集再分成训练集和验证集。训练集是模型的训练数据，验证集部分帮助我们调整模型的参数和其他的一些超参数。\n",
    "3. 线上模式：我们用 debug 模式搭建起一个推荐系统比赛的 baseline，用线下验证模式选择好了模型和一些超参数，这一部分就是真正的对于给定的测试集进行预测，提交到线上，所以这一块使用的训练数据集是全量的数据集（train_click_log+test_click_log）\n",
    "\n",
    "下面就分别对这三种不同的数据读取模式先建立不同的代导入函数，方便后面针对不同的模式下导入数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0c020f9ca28db",
   "metadata": {},
   "source": [
    "debug 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ebf3f8242a902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:41.785154Z",
     "start_time": "2024-11-20T07:10:41.771555Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_click_sample(data_path: str, sample_nums: int=10000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    训练集中采样一部分数据调试。\n",
    "    \n",
    "    Args:\n",
    "        data_path(`str`): 原数据的存储路径\n",
    "        sample_nums(`int`): 采样数目（这里由于机器的内存限制，可以采样用户做）\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 返回一个包含采样后的点击日志数据的 DataFrame；\n",
    "                      该数据只包含随机选择的用户的点击记录，并且去除了重复的点击记录。\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False) \n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942320cd7c79642",
   "metadata": {},
   "source": [
    "根据线上或线下获取对应大小的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b19381367ab44d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:36:23.428981Z",
     "start_time": "2024-11-21T01:36:23.416279Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_click_df(data_path: str='../data/', offline: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    读取点击数据，支持线上和线下数据的获取。\n",
    "    根据  offline 参数的值决定读取训练集或训练集与测试集的合并数据。\n",
    "\n",
    "    Args:\n",
    "        data_path (`str`): 数据存储路径，默认为 './data_raw/'。\n",
    "        offline (`bool`): 是否仅使用训练集数据。若为 True，则只读取训练集；若为 False，则合并训练集和测试集。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 返回一个包含用户点击日志的 DataFrame，数据中去除了重复的点击记录。\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    if not offline:\n",
    "        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "        all_click = pd.concat([all_click, tst_click], ignore_index=True)\n",
    "    \n",
    "    all_click = all_click.drop_duplicates(['user_id', 'click_article_id', 'click_timestamp'])\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8add809db73a1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:41.815177Z",
     "start_time": "2024-11-20T07:10:41.802984Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取 articles.csv 数据集\n",
    "def get_item_info_df(data_path):\n",
    "    item_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    return item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567d4c478954426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:41.830440Z",
     "start_time": "2024-11-20T07:10:41.817266Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取文章的 Embedding 数据集，articles_emb.csv\n",
    "def get_item_emb_dict(data_path):\n",
    "    item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n",
    "    # 进行归一化\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "\n",
    "    item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n",
    "    pickle.dump(item_emb_dict, open('item_content_emb.pkl', 'wb'))\n",
    "    return item_emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dceaf99c93c46",
   "metadata": {},
   "source": [
    "在Python中，pickle模块用于序列化和反序列化Python对象结构。pickle文件是一种二进制文件，可以保存几乎所有的Python对象，包括列表、字典、自定义类等。通过pickle，我们可以方便地将Python对象保存到文件中，并在需要时将其加载回来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472052b152feb122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:36:25.898646Z",
     "start_time": "2024-11-21T01:36:24.785682Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取全量数据集\n",
    "all_click_df = get_all_click_df(offline=False)\n",
    "# 对时间戳进行线性函数归一化，用于在关联规则的时候计算权重\n",
    "all_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee96050c178da8ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:36:27.462008Z",
     "start_time": "2024-11-21T01:36:27.435809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199999</td>\n",
       "      <td>160417</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199999</td>\n",
       "      <td>5408</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199999</td>\n",
       "      <td>50823</td>\n",
       "      <td>0.019359</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199998</td>\n",
       "      <td>157770</td>\n",
       "      <td>0.019340</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199998</td>\n",
       "      <td>96613</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630628</th>\n",
       "      <td>221924</td>\n",
       "      <td>70758</td>\n",
       "      <td>0.343615</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630629</th>\n",
       "      <td>207823</td>\n",
       "      <td>331116</td>\n",
       "      <td>0.343675</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630630</th>\n",
       "      <td>207823</td>\n",
       "      <td>234481</td>\n",
       "      <td>0.343760</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630631</th>\n",
       "      <td>207823</td>\n",
       "      <td>211442</td>\n",
       "      <td>0.343853</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630632</th>\n",
       "      <td>207823</td>\n",
       "      <td>211401</td>\n",
       "      <td>0.343888</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1630633 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  click_article_id  click_timestamp  click_environment  \\\n",
       "0         199999            160417         0.019350                  4   \n",
       "1         199999              5408         0.019351                  4   \n",
       "2         199999             50823         0.019359                  4   \n",
       "3         199998            157770         0.019340                  4   \n",
       "4         199998             96613         0.019378                  4   \n",
       "...          ...               ...              ...                ...   \n",
       "1630628   221924             70758         0.343615                  4   \n",
       "1630629   207823            331116         0.343675                  4   \n",
       "1630630   207823            234481         0.343760                  4   \n",
       "1630631   207823            211442         0.343853                  4   \n",
       "1630632   207823            211401         0.343888                  4   \n",
       "\n",
       "         click_deviceGroup  click_os  click_country  click_region  \\\n",
       "0                        1        17              1            13   \n",
       "1                        1        17              1            13   \n",
       "2                        1        17              1            13   \n",
       "3                        1        17              1            25   \n",
       "4                        1        17              1            25   \n",
       "...                    ...       ...            ...           ...   \n",
       "1630628                  3         2              1            25   \n",
       "1630629                  3         2              1            25   \n",
       "1630630                  3         2              1            25   \n",
       "1630631                  3         2              1            25   \n",
       "1630632                  3         2              1            25   \n",
       "\n",
       "         click_referrer_type  \n",
       "0                          1  \n",
       "1                          1  \n",
       "2                          1  \n",
       "3                          5  \n",
       "4                          5  \n",
       "...                      ...  \n",
       "1630628                    2  \n",
       "1630629                    1  \n",
       "1630630                    1  \n",
       "1630631                    1  \n",
       "1630632                    1  \n",
       "\n",
       "[1630633 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_click_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57814effab797990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:43.399108Z",
     "start_time": "2024-11-20T07:10:43.215819Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取 articles.csv\n",
    "item_info_df = get_item_info_df(data_path)\n",
    "item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8380bc9b281d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:57.876307Z",
     "start_time": "2024-11-20T07:10:43.401623Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取 articles_emb.csv\n",
    "item_emb_dict = get_item_emb_dict(data_path)\n",
    "len(item_emb_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065bb0212b56ffd",
   "metadata": {},
   "source": [
    "## 3 工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312389bda751a52e",
   "metadata": {},
   "source": [
    "### 获取用户-文章-时间函数\n",
    "\n",
    "这个在基于关联规则的用户协同过滤的时候会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf96bf0350f08fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:15:46.167779Z",
     "start_time": "2024-11-20T07:15:46.161779Z"
    }
   },
   "outputs": [],
   "source": [
    "# 根据点击时间获取用户的点击文章序列   {user1: {item1: time1, item2: time2..}...}\n",
    "def get_user_item_time(click_df):\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    \n",
    "    def make_item_time_pair(df):\n",
    "        return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "    \n",
    "    user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']].apply(lambda x: make_item_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "    \n",
    "    return user_item_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa24946ba3eebb7",
   "metadata": {},
   "source": [
    "### 获取文章-用户-时间函数\n",
    "\n",
    "这个在基于关联规则的文章协同过滤的时候会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e82c9cfede1219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:42:28.523027Z",
     "start_time": "2024-11-20T07:42:28.514027Z"
    }
   },
   "outputs": [],
   "source": [
    "# 根据时间获取商品被点击的用户序列  {item1: {user1: time1, user2: time2...}...}\n",
    "# 这里的时间是用户点击当前商品的时间，好像没有直接的关系。\n",
    "def get_item_user_time_dict(click_df):\n",
    "    def make_user_time_pair(df):\n",
    "        return list(zip(df['user_id'], df['click_timestamp']))\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    item_user_time_df = click_df.groupby('click_article_id')[['user_id', 'click_timestamp']].apply(lambda x: make_user_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'user_time_list'})\n",
    "    \n",
    "    item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n",
    "    return item_user_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401ac9f816ffea2",
   "metadata": {},
   "source": [
    "### 获取历史和最后一次点击\n",
    "\n",
    "这个在评估召回结果， 特征工程和制作标签转成监督学习测试集的时候回用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4514ebd056569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:13:23.205599Z",
     "start_time": "2024-11-20T07:13:23.197600Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_hist_and_last_click(all_click: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    获取每个用户的历史点击和最后一次点击记录。\n",
    "    函数将用户的所有点击按时间排序，并返回历史点击和最后一次点击的两个 DataFrame。\n",
    "    \n",
    "    Args:\n",
    "        all_click (`pd.DataFrame`): 包含用户点击数据的 DataFrame，必须包含 'user_id' 和 'click_timestamp' 列。\n",
    "\n",
    "    Returns:\n",
    "        tuple: 返回一个元组，其中第一个元素是包含历史点击的 DataFrame，第二个元素是包含最后一次点击的 DataFrame。\n",
    "    \"\"\"\n",
    "    # 按用户 ID 和点击时间排序\n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    \n",
    "    # 获取每个用户的最后一次点击记录\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "    # 定义函数以获取用户的历史点击\n",
    "    def hist_func(user_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        获取用户的历史点击记录。\n",
    "        如果用户只有一个点击记录，返回该记录；否则，返回除了最后一次点击以外的所有记录。\n",
    "        \n",
    "        Args:\n",
    "            user_df (`pd.DataFrame`): 当前用户的点击数据 DataFrame。\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 用户的历史点击记录。\n",
    "        \"\"\"\n",
    "        if len(user_df) == 1:\n",
    "            return user_df  # 只有一个点击，直接返回\n",
    "        else:\n",
    "            return user_df[:-1]  # 返回除了最后一次点击以外的所有记录\n",
    "\n",
    "    # 获取每个用户的历史点击记录\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df  # 返回历史点击和最后一次点击"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e2aeb42cdfd1d",
   "metadata": {},
   "source": [
    "### 获取文章属性特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf23639949e026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:57.938813Z",
     "start_time": "2024-11-20T07:10:57.928828Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取文章id对应的基本属性，保存成字典的形式，方便后面召回阶段，冷启动阶段直接使用\n",
    "def get_item_info_dict(item_info_df):\n",
    "    item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "    item_category_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n",
    "    item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n",
    "    item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n",
    "    return item_category_dict, item_words_dict, item_created_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28756268270c21",
   "metadata": {},
   "source": [
    "### 获取用户历史点击的文章信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b29c7794998bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:57.954813Z",
     "start_time": "2024-11-20T07:10:57.941812Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_hist_item_info_dict(all_click):\n",
    "    # 获取user_id对应的用户历史点击文章类型的集合字典\n",
    "    user_hist_item_typs = all_click.groupby('user_id')['category_id'].agg(set).reset_index()\n",
    "    user_hist_item_typs_dict = dict(zip(user_hist_item_typs['user_id'], user_hist_item_typs['category_id']))\n",
    "    \n",
    "    # 获取user_id对应的用户点击文章的集合\n",
    "    user_hist_item_ids_dict = all_click.groupby('user_id')['click_article_id'].agg(set).reset_index()\n",
    "    user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n",
    "    \n",
    "    # 获取user_id对应的用户历史点击的文章的平均字数字典\n",
    "    user_hist_item_words = all_click.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "    user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n",
    "    \n",
    "    # 获取user_id对应的用户最后一次点击的文章的创建时间\n",
    "    all_click_ = all_click.sort_values('click_timestamp')\n",
    "    user_last_item_created_time = all_click_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "    \n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    user_last_item_created_time['created_at_ts'] = user_last_item_created_time[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'],\n",
    "                                                user_last_item_created_time['created_at_ts']))\n",
    "    \n",
    "    return user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322562761685645c",
   "metadata": {},
   "source": [
    "### 获取点击最多的 TopK 个文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc215642547cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:57.970813Z",
     "start_time": "2024-11-20T07:10:57.957812Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_topk_click(click_df: pd.DataFrame, k: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    获取点击次数最多的前 k 个文章 ID。\n",
    "\n",
    "    Args:\n",
    "        click_df (`pd.DataFrame`): 包含点击数据的 DataFrame，必须包含 'click_article_id' 列。\n",
    "        k (`int`): 需要返回的前 k 个文章的数量。\n",
    "\n",
    "    Returns:\n",
    "        `pd.Series`: 返回一个包含前 k 个点击次数最多的文章 ID 的 Series。\n",
    "    \"\"\"\n",
    "    return click_df['click_article_id'].value_counts().index[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afcefbdb1baf7c",
   "metadata": {},
   "source": [
    "### 定义多路召回字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd5f4ca833f1b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:58.176890Z",
     "start_time": "2024-11-20T07:10:57.972812Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取文章的属性信息，保存成字典的形式方便查询\n",
    "item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)\n",
    "len(item_type_dict), len(item_words_dict), len(item_created_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7020f3100397260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:10:58.192904Z",
     "start_time": "2024-11-20T07:10:58.178880Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中\n",
    "user_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n",
    "                           'embedding_sim_item_recall': {},\n",
    "                           'youtubednn_recall': {},\n",
    "                           'youtubednn_usercf_recall': {}, \n",
    "                           'cold_start_recall': {}}\n",
    "user_multi_recall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f8523c9fff241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:11:33.685540Z",
     "start_time": "2024-11-20T07:10:58.194903Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提取最后一次点击作为召回评估，如果不需要做召回评估直接使用全量的训练集进行召回（线下验证模型）\n",
    "# 如果不是召回评估，直接使用全量数据进行召回，不用将最后一次提取出来\n",
    "train_hist_click_df, train_last_click_df = get_hist_and_last_click(all_click_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c299876ee76d8ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:11:33.701534Z",
     "start_time": "2024-11-20T07:11:33.687533Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_hist_click_df), len(train_last_click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30120722314f8b10",
   "metadata": {},
   "source": [
    "### 召回效果评估\n",
    "\n",
    "做完了召回有时候也需要对当前的召回方法或者参数进行调整以达到更好的召回效果，因为召回的结果决定了最终排序的上限，下面也会提供一个召回评估的方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28ef90af5ffd848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:11:33.717541Z",
     "start_time": "2024-11-20T07:11:33.703533Z"
    }
   },
   "outputs": [],
   "source": [
    "def metrics_recall(user_recall_items_dict, trn_last_click_df, topk=50):\n",
    "    \"\"\"\n",
    "    依次评估召回的前 10, 20, 30, 40, 50 个文章中的击中率。\n",
    "\n",
    "    Args:\n",
    "        user_recall_items_dict (`dict`): 用户到物品的推荐字典。\n",
    "        trn_last_click_df (`DataFrame`): 包含用户最后一次点击的文章的数据框。\n",
    "        topk (`int`): 评估的最大召回数量。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 创建用户最后点击的物品字典\n",
    "    last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n",
    "    user_num = len(user_recall_items_dict)  # 用户数量\n",
    "    \n",
    "    for k in range(10, topk + 1, 10):  # 依次评估前 k 个召回结果\n",
    "        hit_num = 0  # 记录击中数量\n",
    "        for user, item_list in user_recall_items_dict.items():\n",
    "            # 获取前 k 个召回的结果\n",
    "            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "            # 检查用户最后点击的文章是否在召回结果中\n",
    "            if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                hit_num += 1  # 如果击中，计数加一\n",
    "        \n",
    "        # 计算击中率\n",
    "        hit_rate = round(hit_num * 1.0 / user_num, 5)\n",
    "        print('topk:', k, ' : ', 'hit_num:', hit_num, 'hit_rate:', hit_rate, 'user_num:', user_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ee25415ff29a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4 计算相似性矩阵\n",
    "\n",
    "这一部分主要是通过协同过滤以及向量检索得到相似性矩阵，相似性矩阵主要分为user2user和item2item，下面依次获取基于itemCF的item2item的相似性矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a15d989b9594d",
   "metadata": {},
   "source": [
    "### itemCF i2i_sim\n",
    "\n",
    "借鉴 KDD2020 的去偏商品推荐，在计算 item2item 相似性矩阵时，使用关联规则，使得计算的文章的相似性还考虑到了:\n",
    "\n",
    "1. 用户点击的时间权重\n",
    "2. 用户点击的顺序权重\n",
    "3. 文章创建的时间权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b997473ef2e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:15:49.557023Z",
     "start_time": "2024-11-20T07:15:49.540023Z"
    }
   },
   "outputs": [],
   "source": [
    "def itemcf_sim(df, item_created_time_dict):\n",
    "    \"\"\"\n",
    "    文章与文章之间的相似性矩阵计算。\n",
    "    基于物品的协同过滤和关联规则来生成文章相似度矩阵。\n",
    "\n",
    "    Args:\n",
    "        df (`DataFrame`): 用户点击数据表。\n",
    "        item_created_time_dict (`dict`): 文章创建时间的字典。\n",
    "\n",
    "    Returns:\n",
    "        dict: 文章与文章的相似性矩阵，键为文章ID，值为与之相似的文章及其相似度。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 获取用户与文章的点击时间字典\n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "    \n",
    "    # 初始化物品相似度字典和物品计数器\n",
    "    i2i_sim = {}\n",
    "    item_cnt = defaultdict(int)\n",
    "    \n",
    "    # 遍历每个用户及其点击的文章时间列表\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        # 对于每个用户的点击记录，计算物品之间的相似度\n",
    "        for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "            item_cnt[i] += 1  # 记录物品被点击的次数\n",
    "            i2i_sim.setdefault(i, {})  # 初始化物品相似度字典\n",
    "            \n",
    "            # 遍历同一用户点击的其他物品\n",
    "            for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                if i == j:\n",
    "                    continue  # 跳过自身比较\n",
    "                \n",
    "                # 考虑文章的正向顺序点击和反向顺序点击    \n",
    "                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                # 位置信息权重，参数可以调节\n",
    "                loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n",
    "                # 点击时间权重，参数可以调节\n",
    "                click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n",
    "                # 文章创建时间的权重，参数可以调节\n",
    "                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                \n",
    "                i2i_sim[i].setdefault(j, 0)  # 初始化相似度值\n",
    "                # 综合考虑多种因素计算文章之间的相似度\n",
    "                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "    \n",
    "    # 归一化相似度矩阵\n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])  # 归一化处理\n",
    "\n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "    \n",
    "    return i2i_sim_  # 返回相似性矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f88f4881055aaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:21:38.526673Z",
     "start_time": "2024-11-20T07:17:45.757892Z"
    }
   },
   "outputs": [],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558e655f3c23f78",
   "metadata": {},
   "source": [
    "### UserCF u2u_sim\n",
    "\n",
    "在计算用户之间的相似度的时候，也可以使用一些简单的关联规则，比如用户活跃度权重，这里将用户的点击次数作为用户活跃度的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77587e3615aab56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:39:38.518825Z",
     "start_time": "2024-11-20T07:39:38.503813Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_activate_degree_dict(all_click_df):\n",
    "    # 得到每个用户点击了多少文章\n",
    "    all_click_df_ = all_click_df.groupby('user_id', as_index=False)['click_article_id'].count()\n",
    "    # 用户活跃度归一化\n",
    "    mm = MinMaxScaler()\n",
    "    all_click_df_['click_article_id'] = mm.fit_transform(all_click_df_[['click_article_id']])\n",
    "    user_activate_degree_dict = dict(zip(all_click_df_['user_id'], all_click_df_['click_article_id']))\n",
    "    return user_activate_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdc7305be44567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:40:56.633187Z",
     "start_time": "2024-11-20T07:40:56.613201Z"
    }
   },
   "outputs": [],
   "source": [
    "def usercf_sim(all_click_df, user_activate_degree_dict):\n",
    "    \"\"\"\n",
    "    用户相似性矩阵计算。\n",
    "    \n",
    "    Args:\n",
    "        all_click_df: 数据表。\n",
    "        user_activate_degree_dict (`dict`): 用户活跃度的字典。\n",
    "\n",
    "    Returns:\n",
    "        `dict`: 用户相似性矩阵。\n",
    "\n",
    "    思路: 基于用户的协同过滤 + 关联规则。\n",
    "    \"\"\"\n",
    "    # 获取物品与用户的点击时间字典\n",
    "    item_user_time_dict = get_item_user_time_dict(all_click_df)\n",
    "    \n",
    "    u2u_sim = {}  # 用户相似性字典\n",
    "    user_cnt = defaultdict(int)  # 用户点击计数\n",
    "\n",
    "    # 遍历每个物品及其对应的用户点击时间列表\n",
    "    for item, user_time_list in tqdm(item_user_time_dict.items()):\n",
    "        for u, click_time in user_time_list:\n",
    "            user_cnt[u] += 1  # 记录用户点击次数\n",
    "            u2u_sim.setdefault(u, {})  # 初始化用户相似性字典\n",
    "\n",
    "            # 计算与其他用户的相似性\n",
    "            for v, click_time in user_time_list:\n",
    "                u2u_sim[u].setdefault(v, 0)  # 初始化相似度值\n",
    "                if u == v:\n",
    "                    continue  # 跳过自身比较\n",
    "\n",
    "                # 用户平均活跃度作为活跃度的权重\n",
    "                activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])\n",
    "                u2u_sim[u][v] += activate_weight / math.log(len(user_time_list) + 1)  # 更新相似度\n",
    "\n",
    "    # 归一化相似性矩阵\n",
    "    u2u_sim_ = u2u_sim.copy()\n",
    "    for u, related_users in u2u_sim.items():\n",
    "        for v, wij in related_users.items():\n",
    "            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])  # 归一化处理\n",
    "\n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(u2u_sim_, open(save_path + 'usercf_u2u_sim.pkl', 'wb'))\n",
    "\n",
    "    return u2u_sim_  # 返回用户相似性矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42525bf4ad62fc82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:41:59.108285Z",
     "start_time": "2024-11-20T07:41:58.965741Z"
    }
   },
   "outputs": [],
   "source": [
    "# 由于 usercf 计算的时候太耗费内存了，这里就不直接运行了\n",
    "# 如果是采样的话是可以运行的\n",
    "user_activate_degree_dict = get_user_activate_degree_dict(all_click_df)\n",
    "user_activate_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b775f5427e659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T07:50:49.313126Z",
     "start_time": "2024-11-20T07:42:37.948739Z"
    }
   },
   "outputs": [],
   "source": [
    "u2u_sim = usercf_sim(all_click_df, user_activate_degree_dict)\n",
    "u2u_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fe362dda32dfc",
   "metadata": {},
   "source": [
    "### item embedding sim\n",
    "\n",
    "使用 Embedding 计算 item 之间的相似度是为了后续冷启动的时候可以获取未出现在点击数据中的文章，后面有对冷启动专门的介绍，这里简单的说一下 faiss。\n",
    "\n",
    "aiss 是 Facebook 的 AI 团队开源的一套用于做聚类或者相似性搜索的软件库，底层是用 C++ 实现。Faiss 因为超级优越的性能，被广泛应用于推荐相关的业务当中.\n",
    "\n",
    "faiss 工具包一般使用在推荐系统中的**向量召回**部分。在做向量召回的时候要么是 u2u、u2i 或者 i2i，这里的 u 和 i 指的是 user 和 item。我们知道在实际的场景中 user 和 item 的数量都是海量的，我们最容易想到的基于向量相似度的召回就是使用两层循环遍历 user 列表或者 item 列表计算两个向量的相似度，但是这样做在面对海量数据是不切实际的，faiss 就是用来加速计算某个查询向量最相似的 topk 个索引向量。\n",
    "\n",
    "**faiss** 查询的原理：\n",
    "\n",
    "faiss 使用了 PCA 和 PQ（Product quantization 乘积量化）两种技术进行向量压缩和编码，当然还使用了其他的技术进行优化，但是 PCA 和 PQ 是其中最核心部分。\n",
    "\n",
    "1. PCA 降维算法细节参考下面这个链接进行学习：[主成分分析（PCA）原理总结](https://www.cnblogs.com/pinard/p/6239403.html)\n",
    "2. PQ 编码的细节参考下面这个链接进行学习：[实例理解 product quantization 算法](http://www.fabwrite.com/productquantization)\n",
    "\n",
    "faiss 使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb100fec6eccd36a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:35:45.224305Z",
     "start_time": "2024-11-21T01:35:45.212776Z"
    }
   },
   "outputs": [],
   "source": [
    "# 向量检索相似度计算\n",
    "# topk指的是每个 item，faiss 搜索后返回最相似的 topk 个 item\n",
    "def embdding_sim(click_df, item_emb_df, save_path, topk):\n",
    "    \"\"\"\n",
    "    基于内容的文章 embedding 相似性矩阵计算。\n",
    "\n",
    "    Args:\n",
    "        click_df: 数据表。\n",
    "        item_emb_df: 文章的 embedding。\n",
    "        save_path: 保存路径。\n",
    "        topk (`int`): 找最相似的 topk 篇文章。\n",
    "\n",
    "    Returns:\n",
    "        `dict`: 文章相似性矩阵。\n",
    "\n",
    "    思路: 对于每一篇文章，基于 embedding 的相似性返回 topk 个与其最相似的文章，\n",
    "    由于文章数量太多，这里用了 faiss 进行加速。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 文章索引与文章 id 的字典映射\n",
    "    item_idx_2_rawid_dict = dict(zip(item_emb_df.index, item_emb_df['article_id']))\n",
    "    \n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)\n",
    "    # 向量进行单位化\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "    \n",
    "    # 建立 faiss 索引\n",
    "    item_index = faiss.IndexFlatIP(item_emb_np.shape[1])\n",
    "    item_index.add(item_emb_np)\n",
    "    # 相似度查询，给每个索引位置上的向量返回 topk 个 item 以及相似度\n",
    "    sim, idx = item_index.search(item_emb_np, topk)  # 返回的是列表\n",
    "    \n",
    "    # 将向量检索的结果保存成原始 id 的对应关系\n",
    "    item_sim_dict = collections.defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_np)), sim, idx)):\n",
    "        target_raw_id = item_idx_2_rawid_dict[target_idx]\n",
    "        # 从 1 开始是为了去掉商品本身，所以最终获得的相似商品只有 topk-1\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]): \n",
    "            rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n",
    "            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "     \n",
    "    # 保存 i2i 相似度矩阵\n",
    "    pickle.dump(item_sim_dict, open(save_path + 'emb_i2i_sim.pkl', 'wb'))   \n",
    "    \n",
    "    return item_sim_dict  # 返回文章相似性矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd6caefee0d136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T01:44:52.018948Z",
     "start_time": "2024-11-21T01:40:53.559522Z"
    }
   },
   "outputs": [],
   "source": [
    "item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "emb_i2i_sim = embdding_sim(all_click_df, item_emb_df, save_path, topk=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19779f9c9e85a22d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5 召回\n",
    "\n",
    "这个就是我们开篇提到的那个问题，36 万篇文章，20 多万用户的推荐，我们又有哪些策略来缩减问题的规模？我们就可以再召回阶段筛选出用户对于点击文章的候选集合，从而降低问题的规模。召回常用的策略：\n",
    "\n",
    "- Youtube DNN 召回\n",
    "- 基于文章的召回\n",
    "    - 文章的协同过滤\n",
    "    - 基于文章 Embedding 的召回\n",
    "- 基于用户的召回\n",
    "    - 用于的协同过滤\n",
    "    - 基于用户 Embedding 的召回\n",
    "\n",
    "上面的各种召回方式一部分在基于用户已经看得文章的基础上去召回与这些文章相似的一些文章，而这个相似性的计算方式不同，就得到了不同的召回方式，比如文章的协同过滤，文章内容的embedding等。\n",
    "\n",
    "还有一部分是根据用户的相似性进行推荐，对于某用户推荐与其相似的其他用户看过的文章，比如用户的协同过滤和用户embedding。还有一种思路是类似矩阵分解的思路，先计算出用户和文章的embedding之后，就可以直接算用户和文章的相似度，根据这个相似度进行推荐，比如 YouTube DNN。\n",
    "\n",
    "我们下面详细来看一下每一个召回方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a385cf4a1a0ca2",
   "metadata": {},
   "source": [
    "Youbute DNN 召回\n",
    "\n",
    "这一步直接获取用户召回的候选文章列表：\n",
    "\n",
    "<img src=\"../image/youtube_dnn.png\">\n",
    "\n",
    "关于 Youtube DNN 原理和应用推荐看王喆老师的两篇博客：\n",
    "\n",
    "1. [重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文](https://zhuanlan.zhihu.com/p/52169807)\n",
    "2. [YouTube深度学习推荐系统的十大工程问题](https://zhuanlan.zhihu.com/p/52504407)\n",
    "\n",
    "**参考文献：**\n",
    "\n",
    "1. [YouTubeDNN原理](https://zhuanlan.zhihu.com/p/52169807)\n",
    "2. [word2vec放到排序中的w2v的介绍部分](https://zhuanlan.zhihu.com/p/26306795)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162aadcc23ab68bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:32:53.951482Z",
     "start_time": "2024-11-21T02:32:53.934908Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取双塔召回时的训练验证数据\n",
    "# negsample 指的是通过滑窗构建样本时的负样本数量\n",
    "def gen_data_set(data, negsample=0):\n",
    "    \"\"\"\n",
    "    生成训练和测试数据集。\n",
    "\n",
    "    Args:\n",
    "        data (`DataFrame`): 输入数据，包含用户点击历史。\n",
    "        negsample (`int`): 每个正样本生成的负样本数量。\n",
    "\n",
    "    Returns:\n",
    "        `tuple`: 训练集和测试集的元组。\n",
    "    \"\"\"\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)  # 按点击时间戳排序\n",
    "    item_ids = data['click_article_id'].unique()  # 获取所有文章 ID\n",
    "\n",
    "    train_set = []  # 训练集\n",
    "    test_set = []   # 测试集\n",
    "\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):  # 按用户分组\n",
    "        pos_list = hist['click_article_id'].tolist()  # 用户点击的正样本列表\n",
    "        \n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))  # 用户未点击的文章作为候选负样本\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True)  # 随机选择负样本\n",
    "            \n",
    "        # 当正样本长度为1时，确保该样本被加入训练集和测试集\n",
    "        if len(pos_list) == 1:\n",
    "            train_set.append((reviewerID, [pos_list[0]], pos_list[0], 1, len(pos_list)))\n",
    "            test_set.append((reviewerID, [pos_list[0]], pos_list[0], 1, len(pos_list)))\n",
    "            \n",
    "        # 滑窗构造正负样本\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]  # 用户历史点击的正样本\n",
    "            \n",
    "            if i != len(pos_list) - 1:\n",
    "                # 添加正样本到训练集中\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1])))  # 正样本\n",
    "                for negi in range(negsample):\n",
    "                    # 添加负样本到训练集中\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i * negsample + negi], 0, len(hist[::-1])))  # 负样本\n",
    "            else:\n",
    "                # 将最长的序列长度作为测试数据\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1])))\n",
    "                \n",
    "    random.shuffle(train_set)  # 随机打乱训练集\n",
    "    random.shuffle(test_set)    # 随机打乱测试集\n",
    "    \n",
    "    return train_set, test_set  # 返回训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e250c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入的数据进行padding，使得序列特征的长度都一致\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"click_article_id\": train_iid, \"hist_article_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0567f4c47afe5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtubednn_u2i_dict(data, topk=20):    \n",
    "    \"\"\"\n",
    "    生成用户到物品的推荐字典。\n",
    "\n",
    "    Args:\n",
    "        data (`DataFrame`): 输入数据，包含用户点击历史。\n",
    "        topk (`int`): 每个用户推荐的物品数量。\n",
    "\n",
    "    Returns:\n",
    "        `dict`: 用户到物品的推荐字典。\n",
    "    \"\"\"\n",
    "    sparse_features = [\"click_article_id\", \"user_id\"]\n",
    "    SEQ_LEN = 30  # 用户点击序列的长度，短的填充，长的截断\n",
    "    \n",
    "    user_profile_ = data[[\"user_id\"]].drop_duplicates('user_id')  # 用户画像\n",
    "    item_profile_ = data[[\"click_article_id\"]].drop_duplicates('click_article_id')  # 物品画像\n",
    "    \n",
    "    # 类别编码\n",
    "    features = [\"click_article_id\", \"user_id\"]\n",
    "    feature_max_idx = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feature] = lbe.fit_transform(data[feature])  # 将特征编码为数字\n",
    "        feature_max_idx[feature] = data[feature].max() + 1  # 记录最大索引\n",
    "    \n",
    "    # 提取用户和物品的画像\n",
    "    user_profile = data[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile = data[[\"click_article_id\"]].drop_duplicates('click_article_id')  \n",
    "    \n",
    "    # 创建用户和物品的索引与原始 ID 的映射\n",
    "    user_index_2_rawid = dict(zip(user_profile['user_id'], user_profile_['user_id']))\n",
    "    item_index_2_rawid = dict(zip(item_profile['click_article_id'], item_profile_['click_article_id']))\n",
    "    \n",
    "    # 划分训练和测试集\n",
    "    train_set, test_set = gen_data_set(data, 0)  # 生成训练集和测试集\n",
    "    train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)  # 整理训练数据\n",
    "    test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)  # 整理测试数据\n",
    "    \n",
    "    # 确定 Embedding 的维度\n",
    "    embedding_dim = 16\n",
    "    \n",
    "    # 准备模型输入特征\n",
    "    user_feature_columns = [\n",
    "        SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "        VarLenSparseFeat(SparseFeat('hist_article_id', feature_max_idx['click_article_id'], embedding_dim,\n",
    "                                     embedding_name=\"click_article_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "    ]\n",
    "    item_feature_columns = [SparseFeat('click_article_id', feature_max_idx['click_article_id'], embedding_dim)]\n",
    "    \n",
    "    # 定义模型\n",
    "    model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=5, user_dnn_hidden_units=(64, embedding_dim))\n",
    "    \n",
    "    # 编译模型\n",
    "    model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  \n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(train_model_input, train_label, batch_size=256, epochs=1, verbose=1, validation_split=0.0)\n",
    "    \n",
    "    # 提取训练的 Embedding\n",
    "    test_user_model_input = test_model_input\n",
    "    all_item_model_input = {\"click_article_id\": item_profile['click_article_id'].values}\n",
    "\n",
    "    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "    \n",
    "    # 保存用户和物品的 Embedding\n",
    "    user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "    item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "    \n",
    "    # Embedding 归一化\n",
    "    user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)\n",
    "    item_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)\n",
    "    \n",
    "    # 将 Embedding 转换为字典形式\n",
    "    raw_user_id_emb_dict = {user_index_2_rawid[k]: v for k, v in zip(user_profile['user_id'], user_embs)}\n",
    "    raw_item_id_emb_dict = {item_index_2_rawid[k]: v for k, v in zip(item_profile['click_article_id'], item_embs)}\n",
    "    \n",
    "    # 保存 Embedding 到本地\n",
    "    pickle.dump(raw_user_id_emb_dict, open(save_path + 'user_youtube_emb.pkl', 'wb'))\n",
    "    pickle.dump(raw_item_id_emb_dict, open(save_path + 'item_youtube_emb.pkl', 'wb'))\n",
    "    \n",
    "    # 使用 FAISS 进行相似性搜索，找到与用户最相似的 topk 个物品\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    index.add(item_embs)  # 将物品向量构建索引\n",
    "    sim, idx = index.search(np.ascontiguousarray(user_embs), topk)  # 查询最相似的 topk 个物品\n",
    "    \n",
    "    user_recall_items_dict = collections.defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(test_user_model_input['user_id'], sim, idx)):\n",
    "        target_raw_id = user_index_2_rawid[target_idx]\n",
    "        # 从1开始是为了去掉商品本身，最终获得的相似商品只有 topk-1\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]): \n",
    "            rele_raw_id = item_index_2_rawid[rele_idx]\n",
    "            user_recall_items_dict[target_raw_id][rele_raw_id] = user_recall_items_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "            \n",
    "    # 将召回的结果进行排序\n",
    "    user_recall_items_dict = {k: sorted(v.items(), key=lambda x: x[1], reverse=True) for k, v in user_recall_items_dict.items()}\n",
    "    \n",
    "    # 保存召回结果\n",
    "    pickle.dump(user_recall_items_dict, open(save_path + 'youtube_u2i_dict.pkl', 'wb'))\n",
    "    \n",
    "    return user_recall_items_dict  # 返回用户到物品的推荐字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c819e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:12<00:00, 19404.19it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.compat.v1.squeeze/Squeeze:0', description=\"created by layer 'tf.compat.v1.squeeze'\") of unsupported type <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 由于这里需要做召回评估，所以将训练集中的最后一次点击都提取了出来\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metric_recall:\n\u001b[1;32m----> 3\u001b[0m     user_multi_recall_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutubednn_recall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43myoutubednn_u2i_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_click_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     train_hist_click_df, train_last_click_df \u001b[38;5;241m=\u001b[39m get_hist_and_last_click(all_click_df)\n",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m, in \u001b[0;36myoutubednn_u2i_dict\u001b[1;34m(data, topk)\u001b[0m\n\u001b[0;32m     49\u001b[0m item_feature_columns \u001b[38;5;241m=\u001b[39m [SparseFeat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclick_article_id\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_max_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclick_article_id\u001b[39m\u001b[38;5;124m'\u001b[39m], embedding_dim)]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 定义模型\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYoutubeDNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_feature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_feature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sampled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dnn_hidden_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 编译模型\u001b[39;00m\n\u001b[0;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39msampledsoftmaxloss)  \n",
      "File \u001b[1;32mf:\\anaconda\\anaconda3\\envs\\tf\\lib\\site-packages\\deepmatch\\models\\youtubednn.py:76\u001b[0m, in \u001b[0;36mYoutubeDNN\u001b[1;34m(user_feature_columns, item_feature_columns, num_sampled, user_dnn_hidden_units, dnn_activation, dnn_use_bn, l2_reg_dnn, l2_reg_embedding, dnn_dropout, output_activation, seed)\u001b[0m\n\u001b[0;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_dnn_out)\n\u001b[0;32m     74\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, item_inputs_list)\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 76\u001b[0m                   \u001b[43mget_item_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooling_item_embedding_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem_feature_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mf:\\anaconda\\anaconda3\\envs\\tf\\lib\\site-packages\\deepmatch\\utils.py:35\u001b[0m, in \u001b[0;36mget_item_embedding\u001b[1;34m(item_embedding, item_input_layer)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_item_embedding\u001b[39m(item_embedding, item_input_layer):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem_input_layer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:983\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 983\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1121\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1119\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[0;32m   1120\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1121\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1126\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1127\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:854\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 854\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:899\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m    897\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[0;32m    898\u001b[0m                           build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 899\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeras_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras_tensor_from_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_set_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs:\n\u001b[0;32m    903\u001b[0m   \u001b[38;5;66;03m# TODO(kaftan): figure out if we need to do this at all\u001b[39;00m\n\u001b[0;32m    904\u001b[0m   \u001b[38;5;66;03m# Subclassed network: explicitly set metadata normally set by\u001b[39;00m\n\u001b[0;32m    905\u001b[0m   \u001b[38;5;66;03m# a call to self._set_inputs().\u001b[39;00m\n\u001b[0;32m    906\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_inputs(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py:594\u001b[0m, in \u001b[0;36mkeras_tensor_from_tensor\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    591\u001b[0m     keras_tensor_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 594\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_tensor_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    597\u001b[0m   out\u001b[38;5;241m.\u001b[39m_keras_mask \u001b[38;5;241m=\u001b[39m keras_tensor_from_tensor(tensor\u001b[38;5;241m.\u001b[39m_keras_mask)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py:182\u001b[0m, in \u001b[0;36mKerasTensor.from_tensor\u001b[1;34m(cls, tensor)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m   \u001b[38;5;66;03m# Fallback to the generic arbitrary-typespec KerasTensor\u001b[39;00m\n\u001b[0;32m    181\u001b[0m   name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 182\u001b[0m   type_spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(type_spec, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\type_spec.py:924\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    921\u001b[0m   logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[0;32m    922\u001b[0m       \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a TypeSpec for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    925\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='tf.compat.v1.squeeze/Squeeze:0', description=\"created by layer 'tf.compat.v1.squeeze'\") of unsupported type <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "# 由于这里需要做召回评估，所以将训练集中的最后一次点击都提取了出来\n",
    "if not metric_recall:\n",
    "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(all_click_df, topk=20)\n",
    "else:\n",
    "    train_hist_click_df, train_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(train_hist_click_df, topk=20)\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_recall'], train_last_click_df, topk=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06834763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
